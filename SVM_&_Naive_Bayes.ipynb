{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Support Vector Machine (SVM), and how does it work?   \n",
        "Answer- A Support Vector Machine (SVM) is a supervised machine learning algorithm primarily used for classification but also for regression tasks. Its main goal is to find the optimal boundary, called a hyperplane, that separates data points of different classes in an N-dimensional space (where N is the number of features).\n",
        "\n",
        "How a Support Vector Machine Works\n",
        "The core idea of the SVM algorithm is to find the hyperplane that achieves the maximum margin of separation between the classes.\n",
        "\n",
        "1. Maximizing the Margin\n",
        "Hyperplane: This is the decision boundary. For a 2D problem, it's a line; for a 3D problem, it's a plane; and for more features, it's a hyperplane.\n",
        "\n",
        "Margin: This is the distance between the hyperplane and the closest data point from either class. SVM seeks to maximize this margin because a larger margin is believed to lead to better generalization and a lower chance of overfitting.\n",
        "\n",
        "Support Vectors: These are the data points closest to the hyperplane (the points that \"touch\" the margin). They are the critical elements of the training set because they directly influence the position and orientation of the optimal hyperplane. All other data points can be ignored once the support vectors are identified.\n",
        "\n",
        "2. Handling Non-Linear Data (The Kernel Trick)\n",
        "Real-world data is often not linearly separable, meaning you can't draw a straight line or plane to perfectly separate the classes. SVM handles this non-linear separation using a technique called the Kernel Trick.\n",
        "\n",
        "Mapping to Higher Dimensions: The kernel function implicitly transforms the original non-linear data into a higher-dimensional feature space where the data does become linearly separable. For example, data that's mixed up in 2D might become cleanly separable by a plane in 3D.\n",
        "\n",
        "Classification: Once in the higher-dimensional space, the SVM finds a linear hyperplane to separate the classes. The kernel trick is computationally efficient because it computes the dot products (similarity measures) in the high-dimensional space without ever explicitly performing the costly transformation and calculations in that space. Common kernel functions include Radial Basis Function (RBF), polynomial, and sigmoid.\n",
        "\n",
        "3. Dealing with Overlap (Soft Margin)\n",
        "In cases where the classes overlap or there are outliers, a perfect separation (hard margin) is impossible or leads to poor generalization. In these scenarios, SVM uses a soft margin approach.\n",
        "\n",
        "Slack Variables: A soft margin allows for some data points (misclassifications or points within the margin) to violate the strict separation boundary.\n",
        "\n",
        "Optimization: The SVM algorithm then becomes an optimization problem that balances two competing goals: maximizing the margin and minimizing the misclassification error (controlled by a regularization parameter, C).    \n",
        "\n",
        "Question 2: Explain the difference between Hard Margin and Soft Margin SVM   \n",
        "Hard Margin SVM  \n",
        "Answer- A Hard Margin SVM is used when the data is linearly separable and there are no outliers or noise in the dataset.\n",
        "\n",
        "Feature\tDescription\n",
        "Separation\tStrict and perfect separation of classes.\n",
        "Tolerance\tZero tolerance for misclassification errors or points falling within the margin.\n",
        "Margin Goal\tFind the maximum-width margin that correctly classifies every single training point.\n",
        "Use Case\tIdeal for perfectly clean, linearly separable datasets.\n",
        "Disadvantages\tHighly sensitive to outliers. If even a single point is an outlier, the model may fail to find a separating hyperplane or will find a very narrow margin, leading to overfitting.\n",
        "\n",
        "Export to Sheets\n",
        "The mathematical constraint for a hard margin is: y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b)≥1 for all training points x\n",
        "i\n",
        "​\n",
        " , where y\n",
        "i\n",
        "​\n",
        "  is the class label (±1).\n",
        "\n",
        "Soft Margin SVM\n",
        "A Soft Margin SVM is used for real-world datasets that are often not perfectly linearly separable and contain noise or overlapping data points.\n",
        "\n",
        "Feature\tDescription\n",
        "Separation\tFlexible separation, allowing for some misclassifications or violations of the margin.\n",
        "Tolerance\tIntroduced by slack variables (ξ\n",
        "i\n",
        "​\n",
        " >0), which measure the degree of violation.\n",
        "Optimization\tBalances two objectives: maximizing the margin and minimizing the total training error (misclassifications and margin violations).\n",
        "Parameter\tA regularization parameter (C) controls the trade-off.\n",
        "Use Case\tMost commonly used in practice for non-linearly separable and noisy data.\n",
        "\n",
        "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.   \n",
        "Answer- The Kernel Trick is a powerful mathematical technique used in Support Vector Machines (SVMs) that allows the model to find a non-linear decision boundary without explicitly mapping the data into a high-dimensional feature space.\n",
        "\n",
        "What is the Kernel Trick?\n",
        "The Kernel Trick is a way to handle data that is not linearly separable in its original space.\n",
        "\n",
        "The Goal: The primary objective of an SVM is to find a hyperplane that separates the classes with the maximum margin. For non-linear data (like a circular pattern in 2D), a straight line (hyperplane) can't separate the classes well.\n",
        "\n",
        "\n",
        "The Concept (Feature Mapping): If the original data points are mapped to a much higher-dimensional feature space (often infinitely dimensional), they become linearly separable. However, explicitly performing this transformation ϕ(x) and then computing the dot product ϕ(x\n",
        "i\n",
        "​\n",
        " )⋅ϕ(x\n",
        "j\n",
        "​\n",
        " ) in this high-dimensional space would be computationally very expensive, if not impossible (due to the \"curse of dimensionality\").\n",
        "\n",
        "\n",
        "The Trick: The Kernel Trick uses a kernel function, K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " ), to compute the dot product ϕ(x\n",
        "i\n",
        "​\n",
        " )⋅ϕ(x\n",
        "j\n",
        "​\n",
        " ) directly in the original low-dimensional space, without ever explicitly computing the coordinates of the data points in the high-dimensional space.\n",
        "\n",
        "\n",
        "K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " )=ϕ(x\n",
        "i\n",
        "​\n",
        " )⋅ϕ(x\n",
        "j\n",
        "​\n",
        " )\n",
        "\n",
        "This computational shortcut makes it efficient to train a linear classifier in the high-dimensional space, which corresponds to a non-linear classifier in the original space.\n",
        "\n",
        "Example of a Kernel Function: The Radial Basis Function (RBF) Kernel\n",
        "The Radial Basis Function (RBF) Kernel, also known as the Gaussian kernel, is one of the most popular choices for SVMs.\n",
        "\n",
        "Kernel Formula\n",
        "The formula for the RBF kernel between two data points, x\n",
        "i\n",
        "​\n",
        "  and x\n",
        "j\n",
        "​\n",
        " , is:\n",
        "\n",
        "K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " )=exp(−γ∥x\n",
        "i\n",
        "​\n",
        " −x\n",
        "j\n",
        "​\n",
        " ∥\n",
        "2\n",
        " )\n",
        "\n",
        "Where:\n",
        "\n",
        "∥x\n",
        "i\n",
        "​\n",
        " −x\n",
        "j\n",
        "​\n",
        " ∥\n",
        "2\n",
        "  is the squared Euclidean distance between the two points.\n",
        "\n",
        "γ (gamma) is a hyperparameter that controls the influence of a single training example. A small γ means a large influence (a smooth decision boundary), and a large γ means a small influence (a more complex, wiggly boundary).\n",
        "\n",
        "\n",
        "Use Case\n",
        "The RBF kernel is considered a general-purpose kernel and is highly flexible, as it is capable of mapping the input data into an infinite-dimensional space.\n",
        "\n",
        "When to Use: The RBF kernel is the default and often the first choice when there is no prior knowledge about the data's structure. It's excellent for modeling complex, non-linear, and non-circular relationships in data.\n",
        "\n",
        "\n",
        "How it Works (Intuitively): The kernel measures the similarity of two points. The value of the kernel function decreases as the distance between the two points increases. This effectively creates localized boundaries around the data points.\n",
        "\n",
        "\n",
        "Example Application: It's frequently used in image classification or bioinformatics where the relationships between features are highly complex and not easily defined by simpler polynomial or linear functions.     \n",
        "\n",
        "Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?    \n",
        "Answer- A Naïve Bayes Classifier is a family of simple and fast probabilistic classifiers used in machine learning for classification tasks, such as spam filtering and sentiment analysis. It's a supervised learning algorithm based on Bayes' Theorem.\n",
        "\n",
        "\n",
        "The core of the classifier is to calculate the probability of a given data point belonging to a particular class. It does this by using Bayes' theorem to find the Maximum A Posteriori (MAP) class, which is the class with the highest posterior probability.\n",
        "\n",
        "\n",
        "The formula for Bayes' Theorem in this context is:\n",
        "\n",
        "P(C∣X)=\n",
        "P(X)\n",
        "P(X∣C)P(C)\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "P(C∣X) is the posterior probability: the probability of class C given the feature set X (what we want to find).\n",
        "\n",
        "P(X∣C) is the likelihood: the probability of feature set X given class C.\n",
        "\n",
        "P(C) is the prior probability: the probability of class C before any features are considered.\n",
        "\n",
        "P(X) is the predictor prior probability: the probability of the feature set X.\n",
        "\n",
        "Why is it called \"Naïve\"?\n",
        "The classifier is called \"naïve\" because it makes a strong and often unrealistic assumption about the features in the dataset, known as the \"naïve independence assumption\".\n",
        "\n",
        "This assumption is:\n",
        "\n",
        "Conditional Independence: It assumes that all features (x\n",
        "1\n",
        "​\n",
        " ,x\n",
        "2\n",
        "​\n",
        " ,…,x\n",
        "n\n",
        "​\n",
        " ) are conditionally independent of each other, given the class variable (C).\n",
        "\n",
        "In simpler terms, the model assumes that the presence or absence of one feature has absolutely no influence on the presence or absence of any other feature, as long as the class is known.\n",
        "\n",
        "For example, when classifying a fruit as an apple based on features like 'red color,' 'round shape,' and 'sweet taste,' the Naïve Bayes classifier assumes that the 'red color' is independent of the 'round shape,' even though these features are often correlated in reality.\n",
        "\n",
        "Despite this oversimplified assumption, the algorithm is highly effective, especially for text classification, because this simplification makes the computation much faster and requires less training data to estimate the necessary parameters.     \n",
        "\n",
        "Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
        "When would you use each one?\n",
        "Answer-\n",
        "The main difference between the Naïve Bayes variants (Gaussian, Multinomial, and Bernoulli) lies in the assumption they make about the probability distribution of the features, which dictates the type of data they are best suited for.\n",
        "\n",
        "Naïve Bayes Variants\n",
        "Variant\tDistribution Assumption\tData Type\tKey Feature Modeling\n",
        "Gaussian\tGaussian (Normal) distribution\tContinuous (e.g., height, temperature)\tUses the mean (μ) and variance (σ\n",
        "2\n",
        " ) of the features for each class.\n",
        "Multinomial\tMultinomial distribution\tDiscrete Counts (e.g., word frequency)\tModels the frequency of events (counts or term frequency) given the class.\n",
        "Bernoulli\tBernoulli distribution\tBinary/Boolean (0 or 1, true or false)\tModels the presence or absence of a feature given the class.\n",
        "\n",
        "Export to Sheets\n",
        "Gaussian Naïve Bayes (GaussianNB)\n",
        "Description: Assumes that continuous features associated with each class are drawn from a Gaussian (Normal) distribution.\n",
        "\n",
        "Mechanism: It estimates the mean and variance of each feature for each class during the training phase and uses the Gaussian Probability Density Function (PDF) to calculate the likelihood of a given data point.\n",
        "\n",
        "Multinomial Naïve Bayes (MultinomialNB)\n",
        "Description: Assumes that the features (which are typically integer counts) are generated from a Multinomial distribution.\n",
        "\n",
        "Mechanism: This model is concerned with how many times an event or feature occurred. It takes into account the count or frequency of a feature (e.g., the number of times a word appears in a document).\n",
        "\n",
        "\n",
        "Bernoulli Naïve Bayes (BernoulliNB)\n",
        "Description: Assumes features are generated from a Bernoulli distribution. This means the feature vector must be binary/boolean.\n",
        "\n",
        "Mechanism: It only models the presence (1) or absence (0) of a feature. The absolute count/frequency of the feature does not matter; it only checks if the feature is present or not.\n",
        "\n",
        "\n",
        "Use Cases\n",
        "When to Use Gaussian Naïve Bayes\n",
        "Use Gaussian Naïve Bayes when your features are continuous numerical values and you suspect (or are willing to assume) that they follow a normal distribution.\n",
        "\n",
        "Example Applications:\n",
        "\n",
        "Medical Diagnosis: Classifying a disease based on continuous parameters like blood pressure, age, or heart rate.\n",
        "\n",
        "Predicting House Prices: Using features like square footage, number of rooms, and lot size.\n",
        "\n",
        "Any dataset with purely numerical, non-count-based features.\n",
        "\n",
        "When to Use Multinomial Naïve Bayes\n",
        "Use Multinomial Naïve Bayes when your features are discrete counts representing the number of times a certain event occurred. It is the gold standard for text classification where the features are word counts or term frequencies.\n",
        "\n",
        "Example Applications:\n",
        "\n",
        "Spam Filtering: Classifying an email as \"spam\" or \"not spam\" based on the frequency of certain words like \"free\" or \"viagra.\"\n",
        "\n",
        "Document Classification: Categorizing a document by topic (e.g., sports, finance) based on the counts of relevant words.\n",
        "\n",
        "Sentiment Analysis: Determining if a product review is \"positive\" or \"negative\" based on the frequency of positive/negative words.\n",
        "\n",
        "When to Use Bernoulli Naïve Bayes\n",
        "Use Bernoulli Naïve Bayes when your features are binary (Boolean) variables that indicate the presence or absence of a feature, and the counts or magnitude of the feature's value are not important.\n",
        "\n",
        "Example Applications:\n",
        "\n",
        "Text Classification (Presence only): Classifying a document based only on whether a word is present (1) or absent (0), ignoring how many times it appears.\n",
        "\n",
        "Feature Existence: Any classification problem where the features are simple \"Yes/No\" or \"True/False\" indicators (e.g., Did a customer click the ad? Yes/No).\n",
        "\n",
        "Short Documents: It can sometimes outperform MultinomialNB on shorter, highly sparse documents where word frequency isn't as reliable a signal as simple presence.      \n",
        "\n",
        "Question 6: Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors.   \n",
        "Answer-\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bcl3yBA3xqyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# --- 1. Load the Iris dataset ---\n",
        "# The Iris dataset is a classic classification benchmark with 150 samples\n",
        "# of iris flowers and 4 features.\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "target_names = iris.target_names\n",
        "\n",
        "print(f\"Dataset loaded: {len(X)} samples with {len(feature_names)} features.\")\n",
        "print(f\"Features: {feature_names}\")\n",
        "print(f\"Classes: {target_names}\\n\")\n",
        "\n",
        "# Split the data into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# --- 2. Train an SVM Classifier with a linear kernel ---\n",
        "# SVC (Support Vector Classification) is used for classification tasks.\n",
        "# We explicitly set kernel='linear' for a linear boundary.\n",
        "print(\"Training Linear SVM Classifier...\")\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Train the model using the training data\n",
        "svm_model.fit(X_train, y_train)\n",
        "print(\"Training complete.\\n\")\n",
        "\n",
        "# --- 3. Print the model's accuracy and support vectors ---\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"--- Model Performance ---\")\n",
        "print(f\"Classification Accuracy (on test set): {accuracy:.4f}\")\n",
        "\n",
        "# The support vectors are the subset of training data points\n",
        "# that lie closest to the decision boundary (hyperplane).\n",
        "# They are crucial in defining the boundary.\n",
        "support_vectors = svm_model.support_vectors_\n",
        "\n",
        "print(\"\\n--- Support Vectors (Subset of Training Data) ---\")\n",
        "print(f\"Total number of support vectors: {support_vectors.shape[0]}\")\n",
        "print(\"First 5 Support Vectors (Feature values: Sepal Length, Sepal Width, Petal Length, Petal Width):\")\n",
        "\n",
        "# Print the first 5 support vectors to keep the output clean\n",
        "for i, sv in enumerate(support_vectors[:5]):\n",
        "    # Note: The output format shows raw feature values\n",
        "    print(f\"  SV {i+1}: {sv}\")\n",
        "\n",
        "# Also print the indices of the support vectors in the original training set\n",
        "# print(\"\\nIndices of all Support Vectors in the Training Set:\")\n",
        "# print(svm_model.support_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o95VIKCJ1sfs",
        "outputId": "7efdbc3b-1961-4d9c-99c1-d78f325bb1cf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded: 150 samples with 4 features.\n",
            "Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "Classes: ['setosa' 'versicolor' 'virginica']\n",
            "\n",
            "Training Linear SVM Classifier...\n",
            "Training complete.\n",
            "\n",
            "--- Model Performance ---\n",
            "Classification Accuracy (on test set): 1.0000\n",
            "\n",
            "--- Support Vectors (Subset of Training Data) ---\n",
            "Total number of support vectors: 24\n",
            "First 5 Support Vectors (Feature values: Sepal Length, Sepal Width, Petal Length, Petal Width):\n",
            "  SV 1: [4.8 3.4 1.9 0.2]\n",
            "  SV 2: [5.1 3.3 1.7 0.5]\n",
            "  SV 3: [4.5 2.3 1.3 0.3]\n",
            "  SV 4: [5.6 3.  4.5 1.5]\n",
            "  SV 5: [5.4 3.  4.5 1.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "● Print its classification report including precision, recall, and F1-score."
      ],
      "metadata": {
        "id": "8TSsSvXd1-sB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# --- 1. Load the Breast Cancer dataset ---\n",
        "# This dataset is used for binary classification (malignant vs. benign).\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "target_names = cancer.target_names\n",
        "\n",
        "print(f\"Dataset loaded: {len(X)} samples with {X.shape[1]} features.\")\n",
        "print(f\"Classes: {target_names}\\n\")\n",
        "\n",
        "# Split the data into training and testing sets (70% train, 30% test)\n",
        "# Using random_state for reproducibility\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# --- 2. Train a Gaussian Naïve Bayes model ---\n",
        "# GaussianNB is suitable for continuous features, assuming they follow\n",
        "# a normal (Gaussian) distribution.\n",
        "print(\"Training Gaussian Naïve Bayes Classifier...\")\n",
        "gnb_model = GaussianNB()\n",
        "\n",
        "# Train the model using the training data\n",
        "gnb_model.fit(X_train, y_train)\n",
        "print(\"Training complete.\\n\")\n",
        "\n",
        "# --- 3. Print the classification report ---\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gnb_model.predict(X_test)\n",
        "\n",
        "# Calculate and print overall accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"--- Model Performance Summary ---\")\n",
        "print(f\"Overall Accuracy: {accuracy:.4f}\\n\")\n",
        "\n",
        "# Generate the detailed classification report\n",
        "# The target_names are used to label the classes correctly in the report\n",
        "print(\"--- Detailed Classification Report ---\")\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DRd0sqb2C5d",
        "outputId": "62a13675-e94a-4d88-b279-2f0ab793c177"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded: 569 samples with 30 features.\n",
            "Classes: ['malignant' 'benign']\n",
            "\n",
            "Training Gaussian Naïve Bayes Classifier...\n",
            "Training complete.\n",
            "\n",
            "--- Model Performance Summary ---\n",
            "Overall Accuracy: 0.9415\n",
            "\n",
            "--- Detailed Classification Report ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.93      0.90      0.92        63\n",
            "      benign       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "● Print the best hyperparameters and accuracy   "
      ],
      "metadata": {
        "id": "Cjrt50DQ2O8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# --- 1. Load the Wine dataset ---\n",
        "# The Wine dataset is a multi-class classification problem.\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "target_names = wine.target_names\n",
        "\n",
        "print(f\"Dataset loaded: {len(X)} samples with {X.shape[1]} features.\")\n",
        "print(f\"Classes: {target_names}\\n\")\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "# Using random_state for reproducibility\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# --- 2. Define the SVM model and hyperparameter grid ---\n",
        "# We will use the Radial Basis Function (RBF) kernel, which requires tuning C and gamma.\n",
        "svm = SVC(random_state=42)\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['rbf'] # Specifying the RBF kernel\n",
        "}\n",
        "\n",
        "# --- 3. Train the SVM using GridSearchCV ---\n",
        "# GridSearchCV performs an exhaustive search over the specified parameter values.\n",
        "# cv=5 means 5-fold cross-validation will be used on the training set.\n",
        "print(\"Starting GridSearchCV to find best C and gamma...\")\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=svm,\n",
        "    param_grid=param_grid,\n",
        "    scoring='accuracy', # Metric used to evaluate performance\n",
        "    cv=5,\n",
        "    verbose=2,\n",
        "    n_jobs=-1 # Use all available cores\n",
        ")\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(\"\\nGrid Search Complete.\")\n",
        "\n",
        "# --- 4. Print the best results ---\n",
        "\n",
        "print(\"\\n--- Best Hyperparameters Found ---\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# The best_score_ attribute holds the cross-validation score (accuracy)\n",
        "# achieved with the best parameters on the training set.\n",
        "print(\"\\n--- Best Cross-Validation Accuracy ---\")\n",
        "print(f\"{grid_search.best_score_:.4f}\")\n",
        "\n",
        "# To check performance on the held-out test set:\n",
        "# Predict using the best estimator found by GridSearchCV\n",
        "best_svm = grid_search.best_estimator_\n",
        "y_pred_test = best_svm.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "print(\"\\n--- Final Test Set Accuracy (Using Best Model) ---\")\n",
        "print(f\"{test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fw2_tL_2V9c",
        "outputId": "a2cb642a-8608-446e-ce2a-d6a55b829aef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded: 178 samples with 13 features.\n",
            "Classes: ['class_0' 'class_1' 'class_2']\n",
            "\n",
            "Starting GridSearchCV to find best C and gamma...\n",
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "\n",
            "Grid Search Complete.\n",
            "\n",
            "--- Best Hyperparameters Found ---\n",
            "{'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "\n",
            "--- Best Cross-Validation Accuracy ---\n",
            "0.7180\n",
            "\n",
            "--- Final Test Set Accuracy (Using Best Model) ---\n",
            "0.8333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "● Print the model's ROC-AUC score for its predictions.\n"
      ],
      "metadata": {
        "id": "wX0Hbrk52s0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# --- 1. Load the synthetic text dataset (20 Newsgroups) ---\n",
        "# We'll select a subset of categories for faster loading and simpler analysis.\n",
        "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
        "\n",
        "X_train_raw = newsgroups_train.data\n",
        "y_train = newsgroups_train.target\n",
        "X_test_raw = newsgroups_test.data\n",
        "y_test = newsgroups_test.target\n",
        "target_names = newsgroups_train.target_names\n",
        "\n",
        "print(f\"Dataset loaded: {len(X_train_raw)} training samples, {len(X_test_raw)} test samples.\")\n",
        "print(f\"Categories: {target_names}\\n\")\n",
        "\n",
        "# --- 2. Feature Extraction (Vectorization) ---\n",
        "# Convert text documents into numerical feature vectors using TF-IDF.\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train_raw)\n",
        "# Use the trained vectorizer (fit_transform on train, transform on test)\n",
        "X_test_vectorized = vectorizer.transform(X_test_raw)\n",
        "\n",
        "print(f\"Data vectorized. Training features shape: {X_train_vectorized.shape}\")\n",
        "print(f\"Number of unique features (words) extracted: {X_train_vectorized.shape[1]}\\n\")\n",
        "\n",
        "\n",
        "# --- 3. Train a Naïve Bayes Classifier (MultinomialNB for text) ---\n",
        "# MultinomialNB is best suited for count/frequency features like TF-IDF.\n",
        "print(\"Training Multinomial Naïve Bayes Classifier...\")\n",
        "nb_model = MultinomialNB()\n",
        "\n",
        "# Train the model\n",
        "nb_model.fit(X_train_vectorized, y_train)\n",
        "print(\"Training complete.\\n\")\n",
        "\n",
        "\n",
        "# --- 4. Print the model's ROC-AUC score ---\n",
        "# ROC-AUC requires probability estimates, which we get using predict_proba().\n",
        "# For multi-class (4 classes here), we typically use the 'ovr' (one vs rest) strategy\n",
        "# and a metric like 'micro' or 'macro' averaging.\n",
        "\n",
        "# Get the predicted probabilities for each class\n",
        "y_pred_proba = nb_model.predict_proba(X_test_vectorized)\n",
        "\n",
        "# Calculate the micro-averaged ROC-AUC score\n",
        "# 'micro' computes the average ROC-AUC by considering each element of the label indicator matrix (one-hot encoded targets)\n",
        "# as a binary prediction.\n",
        "try:\n",
        "    roc_auc_micro = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='micro')\n",
        "    print(\"--- Model Performance ---\")\n",
        "    print(f\"ROC-AUC Score (Micro-Averaged, One-vs-Rest): {roc_auc_micro:.4f}\")\n",
        "except ValueError as e:\n",
        "    print(f\"Error calculating ROC-AUC: {e}\")\n",
        "    print(\"Ensure all classes are present in both the training and testing sets.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0hlWGOy2w6V",
        "outputId": "77d81d42-cff9-414e-e2ad-2564f2a481ae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded: 2257 training samples, 1502 test samples.\n",
            "Categories: ['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
            "\n",
            "Data vectorized. Training features shape: (2257, 35482)\n",
            "Number of unique features (words) extracted: 35482\n",
            "\n",
            "Training Multinomial Naïve Bayes Classifier...\n",
            "Training complete.\n",
            "\n",
            "--- Model Performance ---\n",
            "ROC-AUC Score (Micro-Averaged, One-vs-Rest): 0.9819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution    \n",
        "\n",
        "Answer- Strategy for Email Spam Classification\n",
        "This report outlines the approach for developing an automated email classifier to differentiate between Spam and Legitimate (Ham) emails, addressing challenges like diverse text, missing data, and class imbalance.\n",
        "\n",
        "1. Data Preprocessing\n",
        "Handling text, diverse vocabulary, and missing data requires a multi-step pipeline.\n",
        "\n",
        "A. Handling Missing Data\n",
        "Missing Text Fields (e.g., subject, body): Before tokenization, replace any NaN or null values in text columns with a placeholder string (e.g., \"NO_SUBJECT\" or \"EMAIL_BODY_EMPTY\"). This allows the vectorizer to treat the absence of content as a feature in itself.\n",
        "\n",
        "Numerical/Categorical Features: If the system includes non-text metadata (e.g., sender reputation score, number of attachments), impute missing values using standard methods (mean/median for numerical, mode for categorical).\n",
        "\n",
        "B. Text Vectorization\n",
        "Given the diverse vocabulary and large scale, TF-IDF (Term Frequency-Inverse Document Frequency) is the standard and highly effective choice.\n",
        "\n",
        "Process:\n",
        "\n",
        "Tokenization & Cleaning: Remove HTML tags, punctuation, and convert text to lowercase.\n",
        "\n",
        "Stop Word Removal: Filter out common words (the, a, is) to focus on meaningful terms.\n",
        "\n",
        "TF-IDF Transformation: Generate a sparse matrix where each row is an email and each column is a unique word, weighted by its importance (rarity) across the entire corpus. This effectively handles diverse vocabulary by giving higher scores to unique or significant spam-related terms.\n",
        "\n",
        "2. Model Choice and Justification\n",
        "We evaluate two strong candidates for high-dimensional, sparse text data: Multinomial Naïve Bayes (MNB) and Support Vector Machines (SVM).\n",
        "\n",
        "Model\n",
        "\n",
        "Pros\n",
        "\n",
        "Cons\n",
        "\n",
        "Justification\n",
        "\n",
        "Multinomial Naïve Bayes (MNB)\n",
        "\n",
        "Extremely fast training/prediction, excellent baseline for text, handles high dimensions well.\n",
        "\n",
        "Assumes feature independence (untrue for language), often slightly lower accuracy than SVM.\n",
        "\n",
        "Recommended Baseline: Use MNB first. It provides a quick, interpretable, and powerful initial solution.\n",
        "\n",
        "Linear SVM\n",
        "\n",
        "Highly effective in high-dimensional space, finds the optimal separating hyperplane, often superior accuracy to MNB.\n",
        "\n",
        "Slower to train on very large datasets, highly sensitive to hyperparameter tuning and feature scaling.\n",
        "\n",
        "Recommended Final Model: Use Linear SVM after establishing the MNB baseline. Its robust separation capabilities often yield the highest overall performance for this task.\n",
        "\n",
        "The chosen approach is to start with MNB as a production baseline, then optimize and replace it with a Linear SVM for superior accuracy.\n",
        "\n",
        "3. Addressing Class Imbalance\n",
        "Since legitimate emails (Ham) vastly outnumber Spam (the minority class), simply training a model will bias it toward classifying everything as Ham.\n",
        "\n",
        "Primary Strategy: Cost-Sensitive Learning via Class Weighting\n",
        "\n",
        "This is the most direct and least intrusive method for text classification.\n",
        "\n",
        "Implementation: Utilize the class_weight='balanced' parameter (available in scikit-learn's SVM and other models).\n",
        "\n",
        "Mechanism: The model automatically adjusts the weights of the training instances. It assigns a higher penalty (cost) for misclassifying an instance from the minority class (Spam). This forces the model to pay much closer attention to the Spam examples, improving recall.\n",
        "\n",
        "Secondary Strategy: Oversampling (e.g., SMOTE)\n",
        "If class weighting is insufficient, consider synthetic oversampling (SMOTE) on the training data, though this can sometimes introduce noise in high-dimensional text data.\n",
        "\n",
        "4. Performance Evaluation with Suitable Metrics\n",
        "Due to the class imbalance, simple accuracy is inadequate. The focus must be on minimizing the two types of critical errors:\n",
        "\n",
        "Error Type\n",
        "\n",
        "Definition\n",
        "\n",
        "Business Consequence (Cost)\n",
        "\n",
        "False Negative (FN)\n",
        "\n",
        "Spam classified as Ham (Missed Spam)\n",
        "\n",
        "High: User sees spam/phishing/malware. Time wasted. Security risk.\n",
        "\n",
        "False Positive (FP)\n",
        "\n",
        "Ham classified as Spam (Blocked Legitimate Email)\n",
        "\n",
        "Medium/High: Lost business, missed important communication, user frustration.\n",
        "\n",
        "Recommended Metrics:\n",
        "Recall (of the Spam Class):  \n",
        "True Spam+False Negatives\n",
        "True Spam\n",
        "​\n",
        " . This is the most crucial metric as we want to catch as much spam as possible (minimize FN).\n",
        "\n",
        "Precision (of the Spam Class):  \n",
        "True Spam+False Positives\n",
        "True Spam\n",
        "​\n",
        " . This ensures that when the system flags an email as spam, it is highly likely to be correct (minimizing FP).\n",
        "\n",
        "F1-Score: The harmonic mean of Precision and Recall. It provides a single score that balances both concerns.\n",
        "\n",
        "ROC-AUC Score: Measures the model's ability to distinguish between classes across all possible thresholds. A high ROC-AUC indicates a strong underlying model.\n",
        "\n",
        "5. Business Impact\n",
        "The successful deployment of this solution provides immediate, quantifiable business value:\n",
        "\n",
        "Area\n",
        "\n",
        "Impact Description\n",
        "\n",
        "Security and Trust\n",
        "\n",
        "Reduced Risk: Blocks phishing and malware attempts, protecting corporate and user data. Increased Trust: Users rely on the email service knowing critical legitimate emails (Ham) won't be wrongly quarantined.\n",
        "\n",
        "Productivity\n",
        "\n",
        "Time Savings: Employees spend less time manually filtering or deleting junk mail, allowing them to focus on core tasks.\n",
        "\n",
        "Resource Efficiency\n",
        "\n",
        "Optimized Storage: Reduced need to store vast amounts of unwanted spam data on servers.\n",
        "\n",
        "User Experience (UX)\n",
        "\n",
        "Higher Satisfaction: A cleaner, more reliable inbox environment leads to greater user satisfaction and less platform abandonment."
      ],
      "metadata": {
        "id": "c27hR0pW3CmZ"
      }
    }
  ]
}